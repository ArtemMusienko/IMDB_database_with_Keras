{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Решение основной задачи:\n",
        "\n",
        "Для начала необходимо импортировать необходимые библиотеки:"
      ],
      "metadata": {
        "id": "Mpzf7YWgakWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "APxeFiZeofwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее необходимо загрузить набор данных IMDB, указав `num_words=10000`, который указывает, что нужно оставить только 10 000 самых частых слов в отзывах."
      ],
      "metadata": {
        "id": "Qd0VqJzuoilC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000 #устанавливаем размер словаря\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words) #загрузка данных"
      ],
      "metadata": {
        "id": "JLF94ZMXpJfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7282cb7-80da-4f47-8ec5-a2f5b6996f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее выполним бинарную векторизацию текстовых данных (в данном случае — отзывов IMDB) с помощью метода **one-hot encoding**:"
      ],
      "metadata": {
        "id": "UgDp5PW9pRB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in x_train[0]])"
      ],
      "metadata": {
        "id": "tFUsqbl-mcoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c3bb7a-5c69-44a4-ba79-fdc6ab0e8ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию, которая выполняет векторизацию данных - преобразование в бинарные векторы:"
      ],
      "metadata": {
        "id": "BzX1RLAv1wFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension)) #cоздаём нулевую матрицу размером (число отзывов × dimension)\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results"
      ],
      "metadata": {
        "id": "idhc3A4GpRV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготавливаем данные для обучения. Применяем функцию **vectorize_sequences**, преобразовываем метки в тип данных **float32**:"
      ],
      "metadata": {
        "id": "8Wzv2pYPpVSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorize_sequences(x_train)\n",
        "x_test = vectorize_sequences(x_test)\n",
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_test = np.asarray(y_test).astype('float32')"
      ],
      "metadata": {
        "id": "FDf1_uz-pVj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нормализуем входные данные, поделив их на самое максимальное значение их набора данных. Это делается для для масштабирования входных признаков в диапазон [0, 1]."
      ],
      "metadata": {
        "id": "4hrJlBQHlEnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#нормализация входных данных\n",
        "x_train = x_train / x_train.max()\n",
        "x_test = x_test / x_test.max()"
      ],
      "metadata": {
        "id": "_B7GeDAjlFCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее вычислим веса классов (**class weights**) для борьбы с дисбалансом классов в данных:"
      ],
      "metadata": {
        "id": "wcB3HqV0pXV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#вычисление весов классов\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "990oSXwkmwL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем создадим последовательную (**Sequential**) модель нейронной сети для бинарной классификации текстов. Функциями активации будут `relu` и `sigmoid`. Также будем использовать `BatchNormalization()`. Этот метод , который позволяет повысить производительность и стабилизировать работу модели. К первым трём слоям Dense применим l2-регуляризацию с коэффициентом 0.002. Также применим прореживание. Прореживание, которое применяется к слою, заключается в удалении (присваивании нуля) случайно выбираемым признакам на этапе обучения."
      ],
      "metadata": {
        "id": "4yNUcAwwqbSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.002), input_shape=(max_words,)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.7),\n",
        "\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.6),\n",
        "\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_Sk-pC4pXj5",
        "outputId": "faeafacf-6de3-4120-abf8-e66ea3835e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим оптимизатор **Adam**, установив параметр `learning_rate=0.0002`, отвечающий за шаги обучения. Так же начнем компилировать модель, установив параметр функции потерь для бинарной классификации, оптимизатор и метрику:"
      ],
      "metadata": {
        "id": "a4lqwIzbbOgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.0002) #создание оптимизатора\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) #компиляция модели"
      ],
      "metadata": {
        "id": "q9IChVhDbO2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EarlyStopping** - ранняя остановка обучения. Это нужно, чтобы автоматически останавливать обучение, когда модель перестаёт улучшаться, предотвращая переобучение и экономя время. Мониторит метрику `val_accuracy` (точность на валидационных данных). Если точность не улучшается в течение `patience=10` эпох — обучение останавливается. `restore_best_weights=True` — после остановки модель возвращает веса, которые давали наилучшую `val_accuracy`, а не последние веса.\n",
        "\n",
        "**ReduceLROnPlateau** - уменьшение **learning rate**. Мониторит `val_loss` (функцию потерь на валидации). Если ошибка не уменьшается в течение patience=5 эпох:\n",
        "*   Текущий learning rate умножается на factor=0.3;\n",
        "*   Если улучшений нет после нескольких уменьшений, learning rate не упадёт ниже min_lr=1e-6."
      ],
      "metadata": {
        "id": "wHnUFbT-pdsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True) #callback-функция EarlyStopping\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6) #callback-функция ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "YuXbEOrSpd8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее переходим к обучении модели. Установим 80 эпох, количество обучающих примеров, используемых за одну итерацию при обучении нейронной сети = 128, 20% данных уйдёт на валидационную выборку:"
      ],
      "metadata": {
        "id": "n7HPdulvpfLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=80,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "WbiRS4T7pfgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f96a30-d95a-4168-df04-1a63be3c7c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 79ms/step - accuracy: 0.9237 - loss: 0.9056 - val_accuracy: 0.8904 - val_loss: 0.9444 - learning_rate: 2.0000e-04\n",
            "Epoch 2/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 78ms/step - accuracy: 0.9334 - loss: 0.8123 - val_accuracy: 0.8872 - val_loss: 0.8963 - learning_rate: 2.0000e-04\n",
            "Epoch 3/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 72ms/step - accuracy: 0.9380 - loss: 0.7418 - val_accuracy: 0.8852 - val_loss: 0.8567 - learning_rate: 2.0000e-04\n",
            "Epoch 4/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - accuracy: 0.9458 - loss: 0.6791 - val_accuracy: 0.8870 - val_loss: 0.8283 - learning_rate: 2.0000e-04\n",
            "Epoch 5/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 74ms/step - accuracy: 0.9541 - loss: 0.6177 - val_accuracy: 0.8840 - val_loss: 0.8105 - learning_rate: 2.0000e-04\n",
            "Epoch 6/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 75ms/step - accuracy: 0.9543 - loss: 0.5753 - val_accuracy: 0.8846 - val_loss: 0.7894 - learning_rate: 2.0000e-04\n",
            "Epoch 7/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.9615 - loss: 0.5279 - val_accuracy: 0.8820 - val_loss: 0.7689 - learning_rate: 2.0000e-04\n",
            "Epoch 8/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 74ms/step - accuracy: 0.9608 - loss: 0.5048 - val_accuracy: 0.8860 - val_loss: 0.7539 - learning_rate: 2.0000e-04\n",
            "Epoch 9/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - accuracy: 0.9647 - loss: 0.4745 - val_accuracy: 0.8820 - val_loss: 0.7428 - learning_rate: 2.0000e-04\n",
            "Epoch 10/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.9645 - loss: 0.4477 - val_accuracy: 0.8798 - val_loss: 0.7263 - learning_rate: 2.0000e-04\n",
            "Epoch 11/80\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - accuracy: 0.9681 - loss: 0.4239 - val_accuracy: 0.8810 - val_loss: 0.7161 - learning_rate: 2.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем результат обучения. Для выполнения задачи важно отобразить валидационную и контрольную выборки. [1] используется для получения значения точности (**accuracy**) из результатов метода `model.evaluate()`:"
      ],
      "metadata": {
        "id": "5pu5Krq0phOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_accuracy = model.evaluate(x_train, y_train)[1]\n",
        "test_accuracy = model.evaluate(x_test, y_test)[1]\n",
        "\n",
        "print(f\"Валидационная выборка - {val_accuracy:.4f}\")\n",
        "print(f\"Контрольная выборка - {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "Jl_MTfQTZ9xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a977c74-99a6-4952-d33b-3260195faa95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9683 - loss: 0.7500\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8807 - loss: 0.9612\n",
            "Валидационная выборка - 0.9537\n",
            "Контрольная выборка - 0.8821\n"
          ]
        }
      ]
    }
  ]
}